{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# What you will learn\n\n.. youtube:: pa-1O_T5uZI\n    :width: 100% \n    :align: center \n\n## The problem\n\nEvery year since 2005, the [World Happiness Report](https://worldhappiness.report/ed/2018/) \nhas analysed the results of the Gallup World Poll, \nwhich is carried out in 160 countries (covering 99% of the world\u2019s population). \nThe pollsters contact a random sample of people in each country and ask them over \n100 questions about their income, their health and their family. These questions include the \nfollowing question about happiness:\n\n<img src=\"file://../../images/lesson1/HappyQuestion.png\">\n\nPeople living in different countries give different answers. In the UK it is 6.94, making the UK 17th in the world for happiness. \nThe top ranked country --- rather surprisingly given a national stereotype of people who are reserved and don\u2019t express their \nfeelings very much --- is Finland, with a score of 7.82. In general, Scandinavian and Northern European countries are \nranked highest. The USA is 16th (0.03 points ahead of the UK). China, with a score of 5.59 and at 72nd place, is \nroughly in the middle of the table of the countries surveyed. Other mid-ranked countries include Montenegro, Ecuador, \nVietnam and Russia. Further down the table, we find many African --- Uganda and Ethiopia placed 117th and 131st, \nrespectively, Middle Eastern countries --- Iran is at 110 and Yemen at 132.  \nThe unhappiest country in the world in 2022 is Afghanistan, with an average happiness score of only 2.40.\n\nThe question is what makes people happy? One possible answer is that people are happier when they live longer. \nIt is this relationship in data that we will explore in this lesson.\n\n\n## The methods\n\nHere we will learn about plotting and looking for relationships in data;\nfitting straight lines through data points; understanding the slope and intercept of the line \nas parameters in a mathematical model; and showing that the parameters are the best possible fit to the data. \nThese are all key data science skills and also the first steps towards machine learning. Specifically,\nwe will find out more about a method known as linear regression.\n\nBefore we get to linear regression, we are going to go take detour into another part of mathematics: \ncalculus. When you studied calculus at school or university, you probably didn't associate it with finding statsitical\nrelationships in data. But in machine learning, we are often interested in finding the minimum value of a function, and for that \nwe need to go back to differentiation. Once we have done that, \nwe will use differentiation to find the slope of a line which minimises the distances \nbetween points and a line through those points.\n\n## How to use this material\n\nThis material is taught as part of a 6 hour learning session. \nIf you are doing this as part of an organised study group, your Kujenga instructor \nwill have booked a time for an in-person or online two hour session. \nThis means you have two hours to work to do either side of the\nsession. Here is what you should do:\n\n*Before coming to the class*: You should read through this page and get a feeling for the contents and watch the videos. \nAt the section on differentiation, solve the paper and \npencil exercise, but mainly you should read and start thinking about the material. \n\nTo do the computer exercise, you will need to have a Python environment set up on your computer or access via Google Colab (see here for\ninfo on how to set that up). Then you can download this page as a Jupyter notebook or as Python code by clicking the links at\nthe bottom of this page. You should also download the [data](https://github.com/AfricaEuropeCoreAI/Kujenga/blob/main/course/lessons/data/HappinessData.csv) \nfor the exercise. Run the code and focus on understanding what it does up to and including section `Finding the best fit line`_. \n\n*During class*: Your teacher will start by going through the theory for `Finding the best fit line`_. \nPlease ask them questions and actively engage! This is your chance to really understand what is going on.\n\n*After class*: The section `Exercise: look for other predictors of happiness`_ gives the exercises you will need to hand in to\nyour teacher in order to pass the section. Ask your teacher how you should submit your work.\n \n# Differentiation\n\nTaking the derivative of a function is about finding an equation for the slope of the curve the function describes. \nWhen the derivative is zero, the slope is zero. For a recap on differentiation, \n[this page](https://www.bbc.co.uk/bitesize/guides/zyj77ty/revision/1) provides a quick review. And here\nis Blessing from Univeristy of Lagos, Nigeria to lead you through an example!\n\n.. youtube:: dBCV_cYxZAg\n    :width: 100% \n    :align: center \n\n    \n\nIn the example Blessing goes through she tries to find the value of $m$ \nwhich minimises the function $(4-2m)^2$. To solve this problem, \nyou can first multiply out the brackets to get\n\n\\begin{align}(4-2m)^2 = 16 - 16m + 4m^2\\end{align}\n\nYou can then take a derivative in order to calculate the slope of the function, to get\n\n\\begin{align}-16 + 8m\\end{align}\n\n\nWe then solve this equal to zero, because the function is a minimum when it has slope zero.\n\n\\begin{align}- 16 + 8m = 0 \\Rightarrow 8m = 16 \\Rightarrow m = 2\\end{align}\n\nProblem solved. \n\n.. admonition:: Think yourself!\n  \n  Use the derivative to find the minimum of\n\n  .. math::\n\n      (9-3m)^2  \n  \n  (If you get stuck look [here](https://www.bbc.co.uk/bitesize/guides/zyj77ty/revision/1))\n\n\nNote that we use the letter $m$ for the variable, while\nmost often in school we use the letter $x$ for the variable. In maths it really doesn't \nmatter what letter you use, as long as you are consistent, but we will later use $m$ for the slope of a line, \nso we wanted to start using it already now.\n\nIf you can solve the problem above, you have the mathematics needed to work through the rest of this lesson.\nBut, irrespective of whether you can solve the problem above or not, we recommend you have a look at \n[Khan Academy's Calculus 1 course](https://www.khanacademy.org/math/calculus-1). These calculus \nskills are part of the building blocks needed for the Kujenga course.\n      \n# A line through the data\n\nWe already discussed looked at how the [World Happiness Report](https://worldhappiness.report/ed/2018/) \ndocuments the happiness of people across the world. Now, let's load in that data to Python. In this video, \nDavid Sumpter steps through the code. Watch it first then try running the code yourself.\n\n[VIDEO HERE]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\n\n# Read in the data, we shorten the variable names \nhappy = pd.read_csv(\"../data/HappinessData.csv\",delimiter=';') \nhappy.rename(columns = {'Social support':'SocialSupport'}, inplace = True) \nhappy.rename(columns = {'Life Ladder': 'Happiness'}, inplace = True) \nhappy.rename(columns = {'Perceptions of corruption':'Corruption'}, inplace = True) \nhappy.rename(columns = {'Log GDP per capita': 'LogGDP'}, inplace = True) \nhappy.rename(columns = {'Healthy life expectancy at birth': 'LifeExp'}, inplace = True) \nhappy.rename(columns = {'Freedom to make life choices': 'Freedom'}, inplace = True) \n\n# We just look at data for 2018 and dsiplay in table.\ndf=happy.loc[happy['Year'] == 2018]\ndisplay(df[['Country name','LifeExp','Happiness']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the plot \nThe code below plots the average life expectancy of \neach of these countries against their happiness (life ladder) scores. \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pylab import rcParams\nrcParams['figure.figsize'] = 14/2.54, 14/2.54\nmatplotlib.font_manager.FontProperties(family='Helvetica',size=11)\n\ndef plotData(df,x,y): \n    fig,ax=plt.subplots(num=1)\n    ax.plot(x,y, data=df, linestyle='none', markersize=5, marker='o', color=[0.85, 0.85, 0.85])\n    for country in ['United States','United Kingdom','Croatia','Benin','Finland','Yemen']:\n        ci=np.where(df['Country name']==country)[0][0]\n        ax.plot(  df.iloc[ci][x],df.iloc[ci][y], linestyle='none', markersize=7, marker='o', color='black')\n        ax.text(  df.iloc[ci][x]+0.5,df.iloc[ci][y]+0.08,  country)\n           \n    ax.set_xticks(np.arange(30,90,step=5))\n    ax.set_yticks(np.arange(11,step=1))\n    ax.set_ylabel('Average Happiness (0-10)')\n    ax.set_xlabel('Life Expectancy at Birth')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xlim(47,78)\n    ax.set_ylim(2.5,8.1) \n    return fig,ax\n\nfig,ax=plotData(df,'LifeExp','Happiness')\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each circle in the plot is a country. \nThe x-axis shows the life expectancy in the country and \nthe y-axis shows the average ranking of life satisfaction, \non the 0 to 10 scale. In general, the higher the life expectancy of a country, \nthe higher the happiness (life satisfaction) there. \n\n## Drawing a line\n\nOne way to quantify this relationship is to draw a straight line\nthrough the points, showing how happiness increases with life expectancy. \nFor example, imagine that for every 12 extra years which people live in a \ncountry they are one point happier. The equation for happiness in this case \nwould then look like this,\n\n\\begin{align}\\mbox{Happiness} = \\frac{\\mbox{Life Expectancy}}{12}\\end{align}\n\nin this case, if the average life expectancy in the country \nis 60 then the equation above predicts the happiness to be 60/12=5. \nIf the life expectancy is 78 then average happiness is predicted to be 78/12=6.5. And so on...\n\nWe can draw this equation in the form of a straight line going \nthrough the cloud of country points, as shown below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Setup parameters: m is the slope of the line\n# And calculate a line with that slope.\n\nm=1/12\nLife_Expectancy=np.arange(0.5,100,step=0.01)\nHappiness= m*Life_Expectancy\n\n# Plot the data and the line\nfig,ax=plotData(df,'LifeExp','Happiness')\nax.plot(Life_Expectancy, Happiness, linestyle='-', color='black')\ndf=df.assign(Predicted=np.array(m*df['LifeExp']))\nfor country in ['United States','United Kingdom','Croatia','Benin','Finland','Yemen']:\n    ci=np.where(df['Country name']==country)[0][0]\n    ax.plot(  [df.iloc[ci]['LifeExp'],df.iloc[ci]['LifeExp']] ,[ df.iloc[ci]['Happiness'],df.iloc[ci]['Predicted']] ,linestyle=':', color='black')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. admonition:: Try it yourself!\n\n  Download the code by clicking on the link below and \n  try changing the slope and the intercept of the line above by \n  changing the values 1/12 and replotting the line.\n  See if you can find a line that lies closer to the data points.\n\n\n## The sum of squares\n\nEach of the dotted lines above show how far the line \u2013 which predicts that happiness is one \ntwelfth of life expectancy \u2013 is from the data for each of the six highlighted countries.\nFor example, the USA has a happiness score of 6.88 and an \naverage life expectancy of 68.3. The first equation (figure 2b) predicts \n\n\\begin{align}\\mbox{Predicted USA Happiness} = \\frac{\\mbox{USA Life Expectancy}}{12} = \\frac{\\mbox{68.3}}{12} =  5.69\\end{align}\n\nWhich means that the squared distance between the prediction and reality is \n\n\\begin{align}\\end{align}\n\n (6.88 - 5.69)^2 = 1.412\n\nThe table below shows the predicted value and the squared distance between \nprediction and reality for each country. We then sum these squared distances \nto get an overall measure of how far our predictions our from reality. This is done below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df=df.assign(SquaredDistance=np.power((df['Predicted'] - df['Happiness']),2))\ndisplay(df[['Country name','Happiness','Predicted','SquaredDistance']])\n             \nModel_Sum_Of_Squares = np.sum(df['SquaredDistance'])\n\nprint('The model sum of squares is %.4f' % Model_Sum_Of_Squares)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finding the best fit line \nWe have drawn a line. But the question is what the \u2018best\u2019 line is? Blessing goes through the theory \nbelow and then we will calculate the best fitting line for the data above.\n\n.. youtube:: 1dsTGNywCjc\n   :width: 100% \n   :align: center \n\n## Sum of squares\n\nLet\u2019s start by formulating this problem mathematically. \nFor each country $i$, \nwe have two values: the life satisfaction, which I will call $y_i$ \nand life expectancy, which I will call $x_i$ . For example, \nwhen $i=$ USA then $i=x_i=6.88$ and $y_i=68.3$. \n\nNow, let\u2019s denote the slope of the line as $m$ (in the plot above\n$m=1/12$) and repeat the caluclation we made above but with letters instead \nof numbers. First we note that \n\n\\begin{align}\\end{align}\n\n \\hat{y_i} = m \\cdot x_i = 1/12 \\cdot 6.88\n\nThe little \"hat\" in $\\hat{y_i}$ denotes that it is a prediction \n(rather than the measured value itself, which is $y_i$). \nThe squared distance between the prediction and outcome is written as\n\n\\begin{align}\\end{align}\n\n ( y_i - m \\cdot x_i)^2 \n\nAll I am doing is rewriting the same calculation we\ndid above with numbers, but now with the letters. The reason for doing this is that \nour aim is to find an equation for the value of $m$ which minimises the sum of square \ndistances.\n\nThe next step is to write out the sum\n\n\\begin{align}\\end{align}\n\n ( y_1 - m \\cdot x_1)^2 +  ( y_2 - m \\cdot x_2)^2  + ... + ( y_{136} - m \\cdot x_{136})^2  \n\nThe above equation is can be written in shorthand form (using the sum notation we met \nin `the section on our average friend <averagefriends>` as\n\n\\begin{align}\\end{align}\n\n \\sum_i^n ( y_i - m \\cdot x_i)^2 \n\nwhere $n=136$ is the number of countries. \n\n## Back to differentiation\n\nWe want to find the value of $m$ which minimises this sum of squares. But how do we do this? \n\nThe answer is differentiation. We now want to find the value of $m$ which minimises the sum of squares. \nThe equation above is more complicated than the one we used in the section on `Differentiation`_.\n\n\nAlthough  the algebra is more complicated, we can use exactly the same logic to solve the problem \nabove, of finding the value of $m$ which minimises this sum of squares. We first\ntake the derivative \n\n\\begin{align}\\end{align}\n\n \\frac{d}{dm} \\left( ( y_1 - m \\cdot x_1)^2 +  ( y_2 - m \\cdot x_2)^2  + ... + ( y_{136} - m \\cdot x_{136})^2  \\right)\n\n = - 2 x_1 y_1 + 2 x_1^2 m  - 2 x_2 y_2 + 2 x_2^2 m  +  ... - 2 x_{136} y_{136} + 2 x_{136}^2 m  \n\nAlthough this particular step involves alot of algebra, notice that we are doing exactly the same as in the example above.\nAnother thing that that can confuse students is that \nwe differentiate with respect to $m$. \nIn school, we often use the letter $x$ for the variable name and $m$ for a constant. Here it is the other way round. \nThe data $x_i$ and $y_i$ are constants (measurements from countries) and  $m$ is the variable we differentiate for.\n\nWe now write the sum above in shorthand as\n\n\\begin{align}\\end{align}\n\n \\sum_i^n 2 x_i y_i - \\sum_i^n 2 \\cdot x_i^2 m\n\nand we solve equal to zero (to find the point at which it is minimized, and the slope is zero) to get\n\n\\begin{align}\\end{align}\n\n \\sum_i^n 2 x_i y_i - \\sum_i^n 2 \\cdot x_i)^2 m = 0 \\Rightarrow \\sum_i^n 2 x_i y_i = \\sum_i^n 2 \\cdot x_i^2 m \\Rightarrow \\sum_i^n x_i y_i = \\sum_i^n x_i^2\n\nMoving the $m$ to the left hand side gives\n\n\\begin{align}\\end{align}\n\n m = \\frac{\\sum_i^n x_i y_i}{\\sum_i^n x_i^2}\n\nLet's use our newly found equation to calculate the line that best fits the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df=df.assign(SquaredLifEExp=np.power(df['LifeExp'],2))\ndf=df.assign(HappinessLifEExp=df['LifeExp'] * df['Happiness'])\n\nm_best = np.sum(df['HappinessLifEExp'])/np.sum(df['SquaredLifEExp'])\nprint('The best fitting line has slope m = %.4f' % m_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our intial guess of $m = 1/12 = 0.0833$ wasn't so far away from the best fitting value. \nBut this new slope is slightly closer to the data. We can now plot this and recalculate \nthe model sum of squares\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Life_Expectancy=np.arange(0.5,100,step=0.01)\nHappiness= m_best*Life_Expectancy\n\nfig,ax=plotData(df,'LifeExp','Happiness')\nax.plot(Life_Expectancy, Happiness, linestyle='-', color='black')\ndf=df.assign(Predicted=np.array(m_best*df['LifeExp']))\nfor country in ['United States','United Kingdom','Croatia','Benin','Finland','Yemen']:\n    ci=np.where(df['Country name']==country)[0][0]\n    ax.plot(  [df.iloc[ci]['LifeExp'],df.iloc[ci]['LifeExp']] ,[ df.iloc[ci]['Happiness'],df.iloc[ci]['Predicted']] ,linestyle=':', color='black')\n \nplt.show()\n\ndf=df.assign(SquaredDistance=np.power((df['Predicted'] - df['Happiness']),2))\n             \nModel_Sum_Of_Squares = np.sum(df['SquaredDistance'])             \nprint('The model sum of squares is %.4f' % Model_Sum_Of_Squares)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, this sum of squares is slightly smaller than the value we got above \nfor $m = 1/12$ \n\n\n## Including the Intercept\nAn equation for a straight line usually has two components a slope $m$\nwhich we have already seen and an intercept $k$, which so far we have assumed is zero.\nWe can write the equation for a straight line as\n\n\\begin{align}\\end{align}\n\n y = k + m \\times x\n\nWe now look at how we can improve the fit of the model by\nincluding this intercept.\n\nWe start by shifting the data so that it has a mean (average) of zero.\nTo do this we simply take away the mean value from both life expectancy and \nfrom happiness. Then replot the data \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df=df.assign(ShiftedLifeExp=df['LifeExp'] - np.mean(df['LifeExp']))\ndf=df.assign(ShiftedHappiness=df['Happiness'] - np.mean(df['Happiness']))\n\nfig,ax=plotData(df,'ShiftedLifeExp','ShiftedHappiness')\nax.set_ylabel('Happiness (corrected for Mean Happiness)')\nax.set_xlabel('Life Expectancy (corrected for Mean Life Expectancy) ')\nax.set_xticks(np.arange(-30,30,step=5))\nax.set_yticks(np.arange(-5,5,step=1))\nax.set_xlim(-20,15)\nax.set_ylim(-3,3) \nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This graph shows us that, for example, Yemen is almost -2.5 points below the world \naverage for happiness and has a life expectency 8 years shorter than the average over\nall countries in the world. The United States life expectancy is around 3.5 years longer than \nthe average and the citizens of the USA are about 1.3 points happier than average.\nIt is worth noting that the correction is for country averages and does not account for the size of the \npopulations of these various countries. It does however give us a new way \nof seeing between country differences.\n\n\nLet's now try to find the best fit line which goes through these data points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df=df.assign(SquaredLifEExp=np.power(df['ShiftedLifeExp'],2))\ndf=df.assign(HappinessLifEExp=df['ShiftedLifeExp'] * df['ShiftedHappiness'])\n\nm_best = np.sum(df['HappinessLifEExp'])/np.sum(df['SquaredLifEExp'])\nprint('The best fitting line has slope m = %.4f' % m_best)\n\nLife_Expectancy=np.arange(-50,50,step=0.01)\nHappiness= m_best*Life_Expectancy\n\nfig,ax=plotData(df,'ShiftedLifeExp','ShiftedHappiness')\nax.plot(Life_Expectancy, Happiness, linestyle='-', color='black')\nax.set_ylabel('Happiness (corrected for Mean Happiness)')\nax.set_xlabel('Life Expectancy (corrected for Mean Life Expectancy) ')\nax.set_xticks(np.arange(-30,30,step=5))\nax.set_yticks(np.arange(-5,5,step=1))\nax.set_xlim(-20,15)\nax.set_ylim(-3,3) \n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This line appears to fit better than the one we fitted earlier! It lies \ncloser to the points and better capture the relationship in the data.\nTo test whether this is indeed the case we can calculate the sum of squares\nbetween this new line and the shifted data. This is as follows\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df=df.assign(Predicted=np.array(m_best*df['ShiftedLifeExp']))       \ndf=df.assign(SquaredDistance=np.power((df['Predicted'] - df['ShiftedHappiness']),2))\n            \nModel_Sum_Of_Squares = np.sum(df['SquaredDistance'])             \nprint('The model sum of squares is %.4f' % Model_Sum_Of_Squares)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This new line through the data is better! It has a smaller sum of squares. \n\nThe mean values are calculated as follows\n\n\\begin{align}\\end{align}\n\n \\bar{x} = \\frac{1}{n} \\sum_i^n x_i \\mbox{ and }  \\bar{y} = \\frac{1}{n} \\sum_i^n y_i \n\n\nUsing this notation, the equation for the line through the data is\n\n\\begin{align}\\end{align}\n\n \\hat{y_i} - \\bar{y} = m  (\\hat{x_i} - \\bar{x})\n\nJust to remind you about the notation. The predicted value has a hat over it, while the mean values\nhave a bar over them. We can rearrange this equation to get \n\n\\begin{align}\\end{align}\n\n \\hat{y_i}  = m \\hat{x_i} + (\\bar{y} - m\\bar{x})\n\nNotice that this is an equation for a straight line, so we can write\n\n\\begin{align}\\end{align}\n\n \\hat{y_i}  = m \\hat{x_i} + k  \\mbox{ where } k = \\bar{y} - m\\bar{x}\n\nLet's apply this to data and plot the line again\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "k_best = np.mean(df['Happiness']) - m_best*np.mean(df['LifeExp'])\n\nLife_Expectancy=np.arange(0.5,100,step=0.01)\nHappiness= m_best*Life_Expectancy + k_best\n\nfig,ax=plotData(df,'LifeExp','Happiness')\nax.plot(Life_Expectancy, Happiness, linestyle='-', color='black')\ndf=df.assign(Predicted=np.array(m_best*df['LifeExp']+k_best))\nfor country in ['United States','United Kingdom','Croatia','Benin','Finland','Yemen']:\n    ci=np.where(df['Country name']==country)[0][0]\n    ax.plot(  [df.iloc[ci]['LifeExp'],df.iloc[ci]['LifeExp']] ,[ df.iloc[ci]['Happiness'],df.iloc[ci]['Predicted']] ,linestyle=':', color='black')\n \nplt.show()\n\nprint('The slope of the line is m = %.4f and the intercept is k = %.4f' % (m_best,k_best))\nprint('An increase in life expectancy of %.4f years is associated with one extra point of happiness' % (1/m_best))\n   \ndf=df.assign(SquaredDistance=np.power((df['Predicted'] - df['Happiness']),2))          \nModel_Sum_Of_Squares = np.sum(df['SquaredDistance'])             \n\nprint('The model sum of squares is still %.4f' % Model_Sum_Of_Squares)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
<<<<<<< HEAD
        "Now we have it. By shifting back to the original co-ordinates we\ncan find the best fitting line through the data. Notice that the sum of squares is unaffected by\nshifting the line back again, since the distances from the points to the line are unaffected. \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verifying our analytical solution with Statsmodels\n\nSo, we\u2019ve just done all the math by hand,pretty satisfying, right?\nBut now the big question is... could there be an easier way?\nMaybe there\u2019s a Python library that just does all the heavy lifting for us?\n\nYep \u2014 that\u2019s where `statsmodels` comes in. It basically runs the same linear regression\nwe just worked out by hand, but behind the scenes. This is exactly how a Machine\nLearning Engineer would handle it in practice \u2014 less manual math, more letting Python\ndo the work.\n\nWhat we\u2019re doing here is just checking: does `statsmodels` give us the same m and k\nIf they do, that means our manual derivation was spot on!\n\nLet\u2019s test it out \u2014 and Victoria from Kenya will walk you through the code.\n\n*N.B.* Make sure you have `statsmodels` installed in your Python environment:\n   pip install statsmodels  or if you are using **conda**:\n   conda install statsmodels\n\n[VIDEO HERE]\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import the Ordinary Least Squares (OLS) regression tool from statsmodels\n# \"ols\" stands for Ordinary Least Squares \u2013 the most common method for fitting a line.\nfrom statsmodels.formula.api import ols\n\n# Fit a linear regression model using statsmodels.\n# The formula 'Happiness ~ LifeExp' means:\n# \"predict Happiness using LifeExp as the independent variable\".\nmodel = ols('Happiness ~ LifeExp', data=df).fit()\n\n# Display the model parameters:\n# Intercept corresponds to 'k', and the coefficient for LifeExp corresponds to 'm'.\nprint(\"\\nStatsmodels estimated parameters (Intercept and LifeExp):\")\nprint(model.params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can say (roughly speaking) that for every 8 years of life expectancy\ncountry citizens are about 1 point happier on a scale of 0 to 10. It isn't \nthe whole truth (see the word of warning below), but it isn't entirely misleading either. \n\n\n\n# Interpretting the data\n\n\nAlthough there is a relationship between these two variables, this does not mean\nthat life expectancy causes happiness.\n\n\nThe dangers on confusing correlation for causation.....\n\n# Exercise: look for other predictors of happiness\n\n\nFind best predictor using sum of squares\nGive one argument why it causes happiness.\nGive one argument why it might be correlated with but does not cause happiness.\nAdvanced add \n\n"
=======
        "Now we have it. By shifting back to the original co-ordinates we\ncan find the best fitting line through the data. Notice that the sum of squares is unaffected by\nshifting the line back again, since the distances from the points to the line are unaffected. \n\nWe can say (roughly speaking) that for every 8 years of life expectancy\ncountry citizens are about 1 point happier on a scale of 0 to 10. It isn't \nthe whole truth (see the word of warning below), but it isn't entirely misleading either. \n\n\n\n# Interpretting the data\n\n\nAlthough there is a relationship between these two variables, this does not mean\nthat life expectancy causes happiness. One example of a causal relationship between\nlife expectancy and happiness would be one where people are happier because they \nknow they are going to live longer. Less direct, but still causal relationship would\nbe that people are happier because they have access to better healthcare, or \nthat they have less worries about the health of their children and loved ones.\nThere are many seperate factors which can cause both life expectancy and happiness to increase.\nFor example, a country with a high GDP per capita is likely to have both a high life \nexpectancy and a high happiness score (you can test this in the next section). It may even be the case\nthat happy people co-operate more and build better healthcare systems, which in turn leads to longer life expectancy.\n\nJohn Helliwell and his colleagues, who collated the data for the World Happiness report, \nemphasise the importance of, what they call, [social foundations](https://www.tgcom24.mediaset.it/binary/documento/83.$plit/C_2_documento_1063_upfDocumento.pdf#page=10).\nin creating a happier world. \nHappiness arises when we have choices, when the people around us are generous and \nsociable, when we don\u2019t live in poverty and when we are likely to live long lives.  \nBut we have to be careful when interpreting these results. We cannot know, \nfrom cross-country comparison data alone, which factors cause happiness or \nmerely happen to correlate with it. We don\u2019t know if better healthcare or \nbetter social support cause an increase in happiness, or the other way around. \nWhat we do know is that people in more stable, more prosperous countries, with \ngreater social support tend to describe themselves as happier. \n\nThink again about yourself. We cannot rely on national questionnaire results to \nplan our own path to personal happiness. The majority of the factors which \nHelliwell and his colleagues have studied are not under your control. They are \ndetermined by the country you happen to be born in. In some parts of the world \npeople have access to high quality healthcare and do not experience corruption\non an everyday basis. In others they don\u2019t. Access to happiness isn\u2019t entirely a personal \nchoice, it is just a matter of living in the right country at the right time. \n\n\n# Exercise: look for other predictors of happiness\n\nLook at other variables that might predict happiness in the data. You can find the values for\nthese in the dataframe under LogGDP, SocialSupport, Freedom, Generosity and Corruption.\nFull definitions can be found on the [World Happiness Report](https://data.worldhappiness.report/map) website.\n\n\n"
>>>>>>> upstream/main
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose one of the variables and go through the steps we have done for life expectancy above, applying \nthem to your chosen variable. Find a different variable that predicts happiness. Make a plot with\na fitted line through the data.\n\nOnce you have shown the relationship in the data then write \nGive one argument why it might be correlated with but does not cause happiness.\nIn the code below we have plotted the relationship between happiness and perceived corruption in countries, as an example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plotData(df,x,y): \n    fig,ax=plt.subplots(num=1)\n    ax.plot(x,y, data=df, linestyle='none', markersize=5, marker='o', color=[0.85, 0.85, 0.85])\n    for country in ['United States','United Kingdom','Croatia','Benin','Finland','Yemen']:\n        ci=np.where(df['Country name']==country)[0][0]\n        ax.plot(  df.iloc[ci][x],df.iloc[ci][y], linestyle='none', markersize=7, marker='o', color='black')\n        ax.text(  df.iloc[ci][x],df.iloc[ci][y]+0.08,  country)\n\n    ax.set_yticks(np.arange(11,step=1))\n    ax.set_ylabel('Average Happiness (0-10)')\n    ax.set_xlabel(x)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xlim(np.min(df['Corruption']),np.max(df['Corruption']))\n    ax.set_ylim(2.5,8.1) \n    return fig,ax\n\nfig,ax=plotData(df,'Corruption','Happiness')\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using regression in applications\nWe have now seen how to use linear regression to find a line through data points.\nIn the video below we talk to several reasearchers who use linear regression in their work.\n\n.. youtube:: jQZ6db0FzQo\n   :width: 100% \n   :align: center \n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
<<<<<<< HEAD
      "version": "3.14.0"
=======
      "version": "3.13.3"
>>>>>>> upstream/main
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}